{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K    100% |████████████████████████████████| 911kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /home/ritesh/anaconda3/lib/python3.6/site-packages (from selenium) (1.23)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from flask import Flask, render_template, request\n",
    "import os\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobile_prices():\n",
    "    \n",
    "    \n",
    "    # user input of mobile\n",
    "#     def __init__(self):\n",
    "#         self.name = input(\"Enter Mobile Name here: \")\n",
    "        \n",
    "        \n",
    "    # scraping Flipkart\n",
    "    def flipkart(self, product_name):\n",
    "        url = \"https://www.flipkart.com/\"\n",
    "        query = \"search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Flipkart'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.flipkart_details = []\n",
    "        if soup.find_all(class_='_31qSD5'):\n",
    "            for i,mob in enumerate(soup.find_all(class_ = '_31qSD5')):\n",
    "                try:\n",
    "                    name = mob.find(class_ = '_3wU53n').text.strip()\n",
    "                    price = mob.find(class_ = '_1vC4OE _2rQ-NK').text.strip()\n",
    "                    try:\n",
    "                        img_det = re.findall(\"keySpecs(.*?)jpeg\", result.text)[i]\n",
    "                        details = re.findall(\"\\[\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\".*url\\\":\\\"(.*)\", img_det)[0]\n",
    "                        url = details[5]\n",
    "                        url = re.sub(\"{@width}|{@height}\", '250', url) + 'jpeg'\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    try:\n",
    "                        prod_url = mob.attrs['href']\n",
    "                        prod_url = \"https://www.flipkart.com\" + prod_url\n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        rating = mob.find('div', class_ = 'hGSR34 _2beYZw').text.strip()\n",
    "                    except:\n",
    "                        rating = ''\n",
    "                    try:\n",
    "                        no_of_ratings = re.findall('(.*)Ratings',mob.find_all('span', class_ = '_38sUEc')[0].text)[0].strip()\n",
    "                    except:\n",
    "                        no_of_ratings = ''\n",
    "    #                 no_of_reviews = re.findall('\\xa0&\\xa0(.*)Reviews',mob.find_all('span', class_ = '_38sUEc')[0].text)[0].strip()\n",
    "                    self.flipkart_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                     print(site, name, price, url, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            for i,mob in enumerate(soup.find_all('div', class_='_3liAhj _1R0K0g')):\n",
    "                try:\n",
    "                    name = mob.find(class_ = '_2cLu-l').text.strip()\n",
    "                    price = mob.find(class_ = '_1vC4OE').text.strip()\n",
    "                    try:\n",
    "                        img_det = re.findall(\"keySpecs(.*?)jpeg\", result.text)[i]\n",
    "                        details = re.findall(\"\\[\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\".*url\\\":\\\"(.*)\", img_det)[0]\n",
    "                        url = details[5]\n",
    "                        url = re.sub(\"{@width}|{@height}\", '250', url) + 'jpeg'\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    try:\n",
    "                        prod_url = mob.find(class_ = 'Zhf2z-')\n",
    "                        prod_url = prod_url.attrs['href']\n",
    "                        prod_url = \"https://www.flipkart.com\" + prod_url \n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        rating = mob.find(class_ = 'hGSR34 _2beYZw').text.strip()\n",
    "                    except:\n",
    "                        rating = ''\n",
    "                    try:\n",
    "                        no_of_ratings = mob.find(class_ = '_38sUEc').text.strip('()')\n",
    "                    except:\n",
    "                        no_of_ratings = ''\n",
    "                    self.flipkart_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                     print(site, name, price, url, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        return True\n",
    "                \n",
    "    # scraping Snapdeal    \n",
    "    def snapdeal(self, product_name):\n",
    "        url = \"https://www.snapdeal.com/\"\n",
    "        query = \"search?keyword=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Snapdeal'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.snapdeal_details = []\n",
    "        for mob in soup.find_all(class_='col-xs-6'):\n",
    "            try:\n",
    "                name = mob.find(class_ = 'product-title').text.strip()\n",
    "                price = mob.find(class_ = 'lfloat product-price').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find(class_ = 'dp-widget-link')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    rating = re.findall('width:(.*?)\">',str(mob.find(class_ = 'filled-stars')))[0]\n",
    "                except:\n",
    "                    rating = ''\n",
    "                try:\n",
    "                    no_of_ratings = mob.find(class_ = 'product-rating-count').text.strip('()')\n",
    "                except:\n",
    "                    no_of_ratings = ''\n",
    "                try:\n",
    "                    url = mob.find(class_ = 'product-image')\n",
    "                    url = url.attrs['srcset']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.snapdeal_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "            \n",
    "    # scraping PaytmMall    \n",
    "    def paytmmall(self, product_name):\n",
    "        url = \"https://paytmmall.com/\"\n",
    "        query = \"shop/search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        site = 'PaytmMall'\n",
    "        \n",
    "        self.paytmmall_details = []\n",
    "        for mob in soup.find_all('div', class_='_3WhJ'):\n",
    "            try:\n",
    "                name = mob.find(class_ = '_2apC').text.strip()\n",
    "                price = mob.find(class_ = '_1kMS').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find('a')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    prod_url = \"https://paytmmall.com\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    cashback = mob.find(class_ = '_27VV').text.strip()\n",
    "                except:\n",
    "                    cashback = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['src']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.paytmmall_details.append([name, price, cashback, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "    \n",
    "    # scraping ShopClues\n",
    "    def shopclues(self, product_name):\n",
    "        url = \"https://www.shopclues.com/\"\n",
    "        query = \"search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Shopclues'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.shopclues_details = []\n",
    "        for mob in soup.find_all('div', class_='column col3 search_blocks'):\n",
    "            try:\n",
    "                name = mob.find('h2').text.strip()\n",
    "                price = mob.find(class_ = 'p_price').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find('a')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    prod_url = \"http:\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['data-img']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.shopclues_details.append([name, price, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "            \n",
    "    # scraping TataCliq\n",
    "    def tatacliq(self, product_name):\n",
    "        url = \"https://www.tatacliq.com/\"\n",
    "        query = \"search/?text=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'TataCliq'\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "#         options.add_argument(\"--window-size=1000,600\")\n",
    "\n",
    "        # download chromedriver form internet for Chrome selenium and give the path here\n",
    "        CHROMEDRIVER_PATH = 'D:\\chromedriver'\n",
    "        driver = webdriver.Chrome(CHROMEDRIVER_PATH, chrome_options=options)\n",
    "        driver.get(url)\n",
    "        \n",
    "        self.tatacliq_details = []\n",
    "        for mob in driver.find_elements_by_class_name('LK_htgvFpS2PUMylIQlif'):\n",
    "            try:\n",
    "                name = mob.find_elements_by_tag_name('h3')[1].text.strip()\n",
    "                price = mob.find_elements_by_tag_name('h3')[2].text.strip()\n",
    "                self.tatacliq_details.append([name, price, site])\n",
    "#                 print(site, name, price)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "    \n",
    "    # scraping Amazon    \n",
    "    def amazon(self, product_name):\n",
    "#         url = \"https://www.amazon.in/\"\n",
    "        \n",
    "        site = 'Amazon'\n",
    "\n",
    "        url = \"https://www.amazon.in/\"\n",
    "        query = \"s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "        r = requests.get(url, headers = header)\n",
    "        \n",
    "        driver = BeautifulSoup(r.content)\n",
    "        self.amazon_details = []\n",
    "        \n",
    "        for mob in driver.find_all(class_ = 's-item-container'):\n",
    "            try:\n",
    "                name = mob.find('h2', class_ = 'a-size-base').text.strip()\n",
    "                price = mob.find('span', class_ = 'a-size-base').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find(class_ = 'a-link-normal')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    if re.search('url=https', prod_url):\n",
    "                        prod_url = \"https://www.amazon.in\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['src']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.amazon_details.append([name, price, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                try:\n",
    "                    name = mob.find('h2', class_ = 'a-size-medium').text.strip()\n",
    "                    price = mob.find('span', class_ = 'a-size-base').text.strip()\n",
    "                    try:\n",
    "                        prod_url = mob.find(class_ = 'a-link-normal')\n",
    "                        prod_url = prod_url.attrs['href']\n",
    "                        if re.search('url=https', prod_url):\n",
    "                            prod_url = \"https://www.amazon.in\" + prod_url\n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        url = mob.find('img')\n",
    "                        url = url.attrs['src']\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    self.amazon_details.append([name, price, site, url, prod_url])\n",
    "#                     print(site, name, price, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        return True\n",
    "    \n",
    "    # combining and filtering all the scraped results\n",
    "    def combine(self, product_name):\n",
    "        \n",
    "        # regex to remove products containing these keywords\n",
    "        reg = 'cover|case|guard|glass|defender|stand|compatible|combo|accessory|headphone|headset|\\stv\\s|\\sac\\s|led|Refrigerator|washing|keyboard|door|monitor|inverter|machine|kettle|coin|bag|sandal|cable|plug|back'\n",
    "        amazon = pd.DataFrame(self.amazon_details, columns= ['Name', 'Price', 'Site', 'Url', 'Product_Url'])\n",
    "#         tatacliq = pd.DataFrame(self.tatacliq_details, columns= ['Name', 'Price', 'Site'])\n",
    "        shopclues = pd.DataFrame(self.shopclues_details, columns= ['Name', 'Price', 'Site', 'Url', 'Product_Url'])\n",
    "        paytmmall = pd.DataFrame(self.paytmmall_details, columns= ['Name', 'Price', 'Cashback', 'Site', 'Url', 'Product_Url'])\n",
    "        snapdeal = pd.DataFrame(self.snapdeal_details, columns= ['Name', 'Price', 'Rating', 'No of Ratings', 'Site', 'Url', 'Product_Url'])\n",
    "        flipkart = pd.DataFrame(self.flipkart_details, columns= ['Name', 'Price', 'Rating', 'No of Ratings', 'Site', 'Url', 'Product_Url'])\n",
    "#         data = pd.concat([amazon , tatacliq , shopclues, paytmmall, snapdeal, flipkart])\n",
    "        data = pd.concat([amazon, shopclues, paytmmall, snapdeal, flipkart])\n",
    "        \n",
    "        # converting price into string\n",
    "        data.Price = data.Price.astype(str)\n",
    "        \n",
    "        # removing unnecessary symbols from price\n",
    "        data.Price = data.Price.str.replace('₹|Rs.?|,|\\..+','', case = False)\n",
    "        data.Cashback = data.Cashback.str.replace('₹|Rs.?|,|.ashback|\\..+','', case = False)\n",
    "        \n",
    "        # removing products containing certain keywords\n",
    "        data = data[~data.Name.str.contains(reg, case = False)]\n",
    "        \n",
    "        # only keeping products containing our original query\n",
    "        data = data[data.Name.str.contains(product_name, case = False)]\n",
    "        \n",
    "        # converting price into integer from string\n",
    "        data.Price = data.Price.astype(int)\n",
    "        \n",
    "        # removing products having price less than the mean-2*std\n",
    "        data = data[~(data.Price < data.Price.mean() - data.Price.mean()/2)]\n",
    "#         data.to_csv(r'D:\\Innominds\\mobile_prices\\\\' + product_name + '.csv', index = False)\n",
    "        data.sort_values(['Price'], inplace = True)\n",
    "        data = data.head(1)\n",
    "        ans = \"{} on {} for Rs. {}\".format(data.Name.iloc[0], data.Site.iloc[0], str(data.Price.iloc[0]))\n",
    "        img = data.Url.iloc[0]\n",
    "        url = data.Product_Url.iloc[0]\n",
    "        answer = [img, ans, url]\n",
    "#         ans = [data.Name.iloc[0], data.Site.iloc[0], str(data.Price.iloc[0])]\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = mobile_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [05/Feb/2019 09:24:52] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Feb/2019 09:24:52] \"\u001b[33mGET /static/backgroun.jpg HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming header\n",
      "ImmutableMultiDict([('name', 'samsung-nokia-apple')])\n",
      "samsung\n",
      "nokia\n",
      "apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Feb/2019 09:25:16] \"\u001b[37mPOST /hello HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Feb/2019 09:25:16] \"\u001b[33mGET /static/backgroun.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [05/Feb/2019 09:26:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Feb/2019 09:26:05] \"\u001b[33mGET /static/backgroun.jpg HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/hello\", methods=[\"POST\"])\n",
    "def hello():\n",
    "    print(\"coming header\")\n",
    "    if request.method == \"POST\":\n",
    "        ques = request.form\n",
    "        print(ques)\n",
    "        \n",
    "    ques = ques['name'].split('-')\n",
    "    ans = []\n",
    "    for name in ques:\n",
    "        name = name.strip()\n",
    "        print(name)\n",
    "        obj.flipkart(name)\n",
    "        obj.snapdeal(name)\n",
    "        obj.shopclues(name)\n",
    "        obj.paytmmall(name)\n",
    "        obj.amazon(name)\n",
    "#         obj.tatacliq(name)\n",
    "        ans.append(obj.combine(name))\n",
    "    return render_template(\"hello.html\", name = ans)\n",
    "\n",
    "# @app.route(\"/more\")\n",
    "# def more():\n",
    "#     print(an2)\n",
    "#     return render_template(\"more.html\",name=an2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
